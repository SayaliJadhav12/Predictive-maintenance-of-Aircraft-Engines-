{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEG4YjN27gH8"
   },
   "source": [
    "Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 12432,
     "status": "error",
     "timestamp": 1765177029470,
     "user": {
      "displayName": "Aniruddha Kasar",
      "userId": "00560663506505630844"
     },
     "user_tz": -330
    },
    "id": "GqwOvAdllriP",
    "outputId": "13b3a68d-d927-4f3a-baeb-de143c88b8c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "✅ TRAIN dataset shape: (20000, 28)\n",
      "✅ TEST dataset shape: (7000, 25)\n",
      "Loaded 20000 rows, injected 1000 fake units starting from 1.\n",
      "Loaded 7000 rows, injected 500 fake units starting from 2000.\n",
      "Class counts: {0: 8000, 1: 12000} Weights: [1.25, 0.8333333134651184]\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='463' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 463/1500 2:43:22 < 6:07:29, 0.05 it/s, Epoch 0.92/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1️⃣ Install & Import Libraries\n",
    "# !pip install -q transformers accelerate datasets torch pandas scikit-learn tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, T5EncoderModel, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "\n",
    "# -------------------- 1. Load TRAIN & TEST CSV from path --------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Provide full path to your CSV files\n",
    "train_path = \"D:\\Desktop\\POC\\data\\synthetic_balanced_data_20000_60_40 (1).csv\"   # <-- change this to your TRAIN CSV path\n",
    "test_path = \"D:\\Desktop\\POC\\data\\synthetic_balanced_test_data_7000_50_50 (1).csv\"     # <-- change this to your TEST CSV path\n",
    "\n",
    "# Load CSVs\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(\"✅ TRAIN dataset shape:\", train_df.shape)\n",
    "print(\"✅ TEST dataset shape:\", test_df.shape)\n",
    "\n",
    "# --- Column Mapping ---\n",
    "COLUMN_MAP = {f'OpSet{i}': f'op_setting_{i}' for i in range(1,4)}\n",
    "COLUMN_MAP.update({f'Sensor{i}': f'sensor_measurement_{i}' for i in range(1,22)})\n",
    "COLUMN_MAP['Label_RUL_30'] = 'RUL_binary'\n",
    "\n",
    "# --- Load & structure data ---\n",
    "def load_and_structure_data(file_path, total_rows, fake_units, start_unit=1):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.rename(columns=COLUMN_MAP, inplace=True)\n",
    "    CYCLES_PER_UNIT = total_rows // fake_units\n",
    "    df['unit_number'] = np.repeat(range(start_unit, start_unit + fake_units), CYCLES_PER_UNIT)[:total_rows]\n",
    "    df['time_in_cycles'] = np.tile(range(1, CYCLES_PER_UNIT + 1), fake_units)[:total_rows]\n",
    "    df['RUL'] = np.nan\n",
    "    print(f\"Loaded {len(df)} rows, injected {fake_units} fake units starting from {start_unit}.\")\n",
    "    return df\n",
    "\n",
    "df_train = load_and_structure_data(train_path, 20000, 1000)\n",
    "df_test  = load_and_structure_data(test_path, 7000, 500, 2000)\n",
    "\n",
    "# --- Scale Features ---\n",
    "selected_sensors = [2,3,4,7,11,12,15,20,21]\n",
    "feature_cols = [f'op_setting_{i}' for i in range(1,4)] + [f'sensor_measurement_{i}' for i in selected_sensors]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_train[feature_cols] = scaler.fit_transform(df_train[feature_cols])\n",
    "df_test[feature_cols] = scaler.transform(df_test[feature_cols])\n",
    "\n",
    "# --- Class weights ---\n",
    "labels = df_train['RUL_binary']\n",
    "class_counts = labels.value_counts().sort_index()\n",
    "weights = torch.tensor([len(labels)/(2*c) for c in class_counts], dtype=torch.float).to(DEVICE)\n",
    "print(\"Class counts:\", class_counts.to_dict(), \"Weights:\", weights.tolist())\n",
    "\n",
    "# --- Context Dataset ---\n",
    "CONTEXT_LENGTH = 15\n",
    "\n",
    "def create_prompt(context_df):\n",
    "    s = \"\"\n",
    "    for i, row in enumerate(context_df.itertuples()):\n",
    "        cycle_idx = f\"t-{CONTEXT_LENGTH-1-i}\" if i<CONTEXT_LENGTH-1 else \"t\"\n",
    "        sensors_str = \", \".join([f\"s{i+1}={getattr(row, col):.3f}\" for col in feature_cols if 'sensor' in col])\n",
    "        s += f\"Cycle ({cycle_idx}): {sensors_str}\\n\"\n",
    "    return s\n",
    "\n",
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.samples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        for unit_id in df['unit_number'].unique():\n",
    "            unit_df = df[df['unit_number']==unit_id].sort_values('time_in_cycles')\n",
    "            for i in range(CONTEXT_LENGTH-1, len(unit_df)):\n",
    "                ctx = unit_df.iloc[i-CONTEXT_LENGTH+1:i+1]\n",
    "                label = int(ctx['RUL_binary'].iloc[-1])\n",
    "                self.samples.append((ctx, label))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        ctx, label = self.samples[idx]\n",
    "        prompt = create_prompt(ctx)\n",
    "        inputs = tokenizer(prompt, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "        return {'input_ids': inputs['input_ids'].squeeze(),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                'labels': torch.tensor(label)}\n",
    "\n",
    "# --- Tokenizer & Model ---\n",
    "MODEL_NAME = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "class T5BinaryClassifier(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super().__init__()\n",
    "        self.encoder = T5EncoderModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.encoder.config.d_model, 2)\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        x = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        logits = self.classifier(x[:,0,:])\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "model = T5BinaryClassifier().to(DEVICE)\n",
    "\n",
    "# --- Prepare Datasets ---\n",
    "train_dataset = ContextDataset(df_train, tokenizer)\n",
    "eval_dataset  = ContextDataset(df_test, tokenizer)  # Or a validation split from train\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"D:/Desktop/POC/data/t5_rul_binary\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_dir='D:\\Desktop\\POC\\data\\logs',\n",
    "    logging_steps=50,\n",
    "    save_strategy='no',\n",
    "    eval_strategy='epoch'\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# --- Start Training ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# --- Predictions & Metrics ---\n",
    "def evaluate_model(trainer, dataset):\n",
    "    trainer.model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=4)\n",
    "    all_preds, all_labels = [], []\n",
    "    for batch in loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = trainer.model(input_ids, attention_mask=mask)\n",
    "        preds = torch.argmax(outputs['logits'], dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {acc*100:.2f}%\\nPrecision: {prec:.2f}\\nRecall: {rec:.2f}\\nF1: {f1:.2f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\n--- Train Evaluation ---\")\n",
    "evaluate_model(trainer, train_dataset)\n",
    "\n",
    "print(\"\\n--- Test Evaluation ---\")\n",
    "evaluate_model(trainer, eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5887,
     "status": "ok",
     "timestamp": 1759727965073,
     "user": {
      "displayName": "Aniruddha Kasar",
      "userId": "00560663506505630844"
     },
     "user_tz": -330
    },
    "id": "6lR-6618wyS4",
    "outputId": "18492f25-66cf-4033-dff4-d3a00d292c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Unit-wise Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting per unit: 100%|██████████| 500/500 [00:05<00:00, 84.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n",
      "Confusion Matrix (rows: true, cols: predicted):\n",
      " [[250   0]\n",
      " [  0 250]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 16️⃣ Unit-wise Predictions on Test Set ---\n",
    "def predict_unitwise(model, tokenizer, test_df, feature_columns, context_length=15):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for unit_id in tqdm(test_df['unit_number'].unique(), desc=\"Predicting per unit\"):\n",
    "        unit_df = test_df[test_df['unit_number'] == unit_id].sort_values('time_in_cycles')\n",
    "        # Use last CONTEXT_LENGTH cycles as context\n",
    "        context_df = unit_df.tail(context_length) if len(unit_df) >= context_length else unit_df\n",
    "\n",
    "        true_label = int(context_df['RUL_binary'].iloc[-1])\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "        prompt = create_prompt(context_df)\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "            pred = torch.argmax(outputs['logits'], dim=1).item()\n",
    "        predictions.append(pred)\n",
    "\n",
    "    return np.array(true_labels), np.array(predictions)\n",
    "\n",
    "# --- 17️⃣ Evaluate Unit-wise ---\n",
    "print(\"\\n--- Unit-wise Test Set Evaluation ---\")\n",
    "true_ruls, predicted_ruls = predict_unitwise(model, tokenizer, df_test, feature_cols, CONTEXT_LENGTH)\n",
    "\n",
    "# Calculate metrics\n",
    "acc  = accuracy_score(true_ruls, predicted_ruls)\n",
    "prec = precision_score(true_ruls, predicted_ruls, zero_division=0)\n",
    "rec  = recall_score(true_ruls, predicted_ruls, zero_division=0)\n",
    "f1   = f1_score(true_ruls, predicted_ruls, zero_division=0)\n",
    "cm   = confusion_matrix(true_ruls, predicted_ruls)\n",
    "\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Precision: {prec:.2f}\")\n",
    "print(f\"Recall: {rec:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Confusion Matrix (rows: true, cols: predicted):\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.makedirs(\"artifacts/models\", exist_ok=True)\n",
    "\n",
    "# Save trained CNN model (state dict)\n",
    "torch.save(model.state_dict(), \"artifacts/models/llm_model.pt\")\n",
    "\n",
    "print(\"✔ LLM model saved at artifacts/models/llm_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
