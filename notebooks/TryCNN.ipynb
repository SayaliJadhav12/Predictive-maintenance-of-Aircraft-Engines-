{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "tVyGiUZ1nT-B",
        "outputId": "5c4f740f-d109-4683-c235-d4382e189794"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:13: SyntaxWarning: invalid escape sequence '\\D'\n",
            "<>:14: SyntaxWarning: invalid escape sequence '\\D'\n",
            "<>:13: SyntaxWarning: invalid escape sequence '\\D'\n",
            "<>:14: SyntaxWarning: invalid escape sequence '\\D'\n",
            "C:\\Users\\sjadhav\\AppData\\Local\\Temp\\1\\ipykernel_23844\\1186363166.py:13: SyntaxWarning: invalid escape sequence '\\D'\n",
            "  train_path = \"D:\\Desktop\\POC\\data\\synthetic_balanced_data_20000_60_40 (1).csv\"   # <-- change this to your TRAIN CSV path\n",
            "C:\\Users\\sjadhav\\AppData\\Local\\Temp\\1\\ipykernel_23844\\1186363166.py:14: SyntaxWarning: invalid escape sequence '\\D'\n",
            "  test_path = \"D:\\Desktop\\POC\\data\\synthetic_balanced_test_data_7000_50_50 (1).csv\"     # <-- change this to your TEST CSV path\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… TRAIN dataset shape: (20000, 28)\n",
            "âœ… TEST dataset shape: (7000, 25)\n",
            "\n",
            "Train cycles/unit: 20\n",
            "Test cycles/unit : 18\n",
            "Using device: cpu\n",
            "Class distribution: {0: 8000, 1: 12000}\n",
            "\n",
            "Train samples: 6000\n",
            "Test samples : 1554\n",
            "\n",
            "ðŸš€ Training Started...\n",
            "\n",
            "Epoch 1/5  Loss: 0.6828\n",
            "Epoch 2/5  Loss: 0.4877\n",
            "Epoch 3/5  Loss: 0.1672\n",
            "Epoch 4/5  Loss: 0.0686\n",
            "Epoch 5/5  Loss: 0.0350\n",
            "\n",
            "ðŸŽ‰ Training Complete!\n",
            "\n",
            "ðŸ“Œ Test Evaluation\n",
            "Accuracy : 0.9980694980694981\n",
            "Precision: 1.0\n",
            "Recall   : 0.9961340206185567\n",
            "F1 Score : 0.9980632666236281\n",
            "Confusion Matrix:\n",
            " [[778   0]\n",
            " [  3 773]]\n",
            "\n",
            "ðŸ“Œ Unit-wise Evaluation\n",
            "Accuracy : 1.0\n",
            "Precision: 1.0\n",
            "Recall   : 1.0\n",
            "F1 Score : 1.0\n",
            "Confusion Matrix:\n",
            " [[195   0]\n",
            " [  0 194]]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ðŸ”¥ FINAL WORKING CNN MODEL FOR PREDICTIVE MAINTENANCE\n",
        "# ============================================================\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -------------------- 1. Load TRAIN & TEST CSV from path --------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Provide full path to your CSV files\n",
        "train_path = \"D:\\Desktop\\POC\\data\\synthetic_balanced_data_20000_60_40 (1).csv\"   # <-- change this to your TRAIN CSV path\n",
        "test_path = \"D:\\Desktop\\POC\\data\\synthetic_balanced_test_data_7000_50_50 (1).csv\"     # <-- change this to your TEST CSV path\n",
        "\n",
        "# Load CSVs\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "print(\"âœ… TRAIN dataset shape:\", train_df.shape)\n",
        "print(\"âœ… TEST dataset shape:\", test_df.shape)\n",
        "\n",
        "# -------------------- 2. Column Rename + Structure ------------------\n",
        "COLUMN_MAP = {f'OpSet{i}': f'op_setting_{i}' for i in range(1,4)}\n",
        "COLUMN_MAP.update({f'Sensor{i}': f'sensor_measurement_{i}' for i in range(1,22)})\n",
        "COLUMN_MAP['Label_RUL_30'] = 'RUL_binary'\n",
        "\n",
        "def load_and_structure_data(file_path, fake_units):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.rename(columns=COLUMN_MAP, inplace=True)\n",
        "\n",
        "    total_rows = len(df)\n",
        "\n",
        "    # FIX â†’ use ceil so fake_units * cycles >= total_rows\n",
        "    cycles_per_unit = int(np.ceil(total_rows / fake_units))\n",
        "\n",
        "    df['unit_number'] = np.repeat(range(1, fake_units+1), cycles_per_unit)[:total_rows]\n",
        "    df['time_in_cycles'] = np.tile(range(1, cycles_per_unit+1), fake_units)[:total_rows]\n",
        "\n",
        "    return df, cycles_per_unit\n",
        "\n",
        "# IMPORTANT FIX: set fake_units so cycles >= 15\n",
        "df_train, train_cycles = load_and_structure_data(train_path, fake_units=1000)\n",
        "df_test, test_cycles   = load_and_structure_data(test_path,  fake_units=400)\n",
        "\n",
        "print(\"\\nTrain cycles/unit:\", train_cycles)\n",
        "print(\"Test cycles/unit :\", test_cycles)\n",
        "\n",
        "# -------------------- 3. Scaling -------------------------------\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "selected_sensors = [2,3,4,7,11,12,15,20,21]\n",
        "feature_cols = [f'op_setting_{i}' for i in range(1,4)] + [\n",
        "    f'sensor_measurement_{i}' for i in selected_sensors\n",
        "]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_train[feature_cols] = scaler.fit_transform(df_train[feature_cols])\n",
        "df_test[feature_cols] = scaler.transform(df_test[feature_cols])\n",
        "\n",
        "# -------------------- 4. Class Weights -------------------------\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "labels = df_train['RUL_binary']\n",
        "class_counts = labels.value_counts().sort_index()\n",
        "weights = torch.tensor([len(labels)/(2*c) for c in class_counts]).float().to(DEVICE)\n",
        "\n",
        "print(\"Class distribution:\", class_counts.to_dict())\n",
        "\n",
        "# -------------------- 5. Dataset (15-cycle window) --------------\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "CONTEXT_LENGTH = 15\n",
        "\n",
        "class CNNDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.samples = []\n",
        "        for unit in df['unit_number'].unique():\n",
        "            u = df[df['unit_number']==unit].sort_values('time_in_cycles')\n",
        "            for i in range(CONTEXT_LENGTH-1, len(u)):\n",
        "                ctx = u.iloc[i-CONTEXT_LENGTH+1:i+1]\n",
        "                x = ctx[feature_cols].values.astype(np.float32)  # (15,12)\n",
        "                y = int(ctx['RUL_binary'].iloc[-1])\n",
        "                self.samples.append((x, y))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        x = torch.tensor(x).permute(1, 0)   # â†’ [12, 15]\n",
        "        return x, torch.tensor(y)\n",
        "\n",
        "train_dataset = CNNDataset(df_train)\n",
        "test_dataset  = CNNDataset(df_test)\n",
        "\n",
        "print(\"\\nTrain samples:\", len(train_dataset))\n",
        "print(\"Test samples :\", len(test_dataset))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# -------------------- 6. CNN Model -----------------------------\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNNBinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(12, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        return self.fc(x)\n",
        "\n",
        "model = CNNBinaryClassifier().to(DEVICE)\n",
        "\n",
        "# -------------------- 7. Train Loop -----------------------------\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 5\n",
        "print(\"\\nðŸš€ Training Started...\\n\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}  Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Training Complete!\")\n",
        "\n",
        "# -------------------- 8. Evaluation ----------------------------\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    preds, trues = [], []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            out = model(x)\n",
        "            pred = torch.argmax(out, dim=1)\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            trues.extend(y.cpu().numpy())\n",
        "\n",
        "    print(\"\\nðŸ“Œ Test Evaluation\")\n",
        "    print(\"Accuracy :\", accuracy_score(trues, preds))\n",
        "    print(\"Precision:\", precision_score(trues, preds))\n",
        "    print(\"Recall   :\", recall_score(trues, preds))\n",
        "    print(\"F1 Score :\", f1_score(trues, preds))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(trues, preds))\n",
        "\n",
        "evaluate(model, test_loader)\n",
        "\n",
        "# -------------------- 9. Unit-wise Evaluation ------------------\n",
        "def unitwise_predict(model, df):\n",
        "    trues, preds = [], []\n",
        "    model.eval()\n",
        "\n",
        "    for unit in df['unit_number'].unique():\n",
        "        u = df[df['unit_number']==unit].sort_values('time_in_cycles')\n",
        "        ctx = u.tail(CONTEXT_LENGTH)\n",
        "        x = torch.tensor(ctx[feature_cols].values.astype(np.float32)).permute(1,0).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(x)\n",
        "            pred = torch.argmax(out, dim=1).item()\n",
        "\n",
        "        trues.append(int(ctx['RUL_binary'].iloc[-1]))\n",
        "        preds.append(pred)\n",
        "\n",
        "    return np.array(trues), np.array(preds)\n",
        "\n",
        "y_true, y_pred = unitwise_predict(model, df_test)\n",
        "\n",
        "print(\"\\nðŸ“Œ Unit-wise Evaluation\")\n",
        "print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "print(\"Recall   :\", recall_score(y_true, y_pred))\n",
        "print(\"F1 Score :\", f1_score(y_true, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqQuISEJnwZx",
        "outputId": "8fbaa69c-e29a-4f7a-ea9f-832b13f09e72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ” CNN model saved at artifacts/models/cnn_model.pt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.makedirs(\"artifacts/models\", exist_ok=True)\n",
        "\n",
        "# Save trained CNN model (state dict)\n",
        "torch.save(model.state_dict(), \"artifacts/models/cnn_model.pt\")\n",
        "\n",
        "print(\"âœ” CNN model saved at artifacts/models/cnn_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PdrbTvrnXIW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
